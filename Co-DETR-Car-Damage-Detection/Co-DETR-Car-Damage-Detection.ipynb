{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuHEqqRAW4Qe"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Mount Google Drive and Set Dataset Path ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "extract_path = \"/content/drive/MyDrive/Colab Notebooks/CarDD/CarDD_COCO\"\n",
        "\n",
        "# --- Step 2: Install Required Libraries ---\n",
        "!pip install -q timm pycocotools\n",
        "!pip install albumentations --quiet\n",
        "\n",
        "# --- Step 3: Import Libraries ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.ops import generalized_box_iou_loss\n",
        "from torchvision.ops.boxes import box_convert\n",
        "import timm\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "torch._C._jit_set_nvfuser_enabled(False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Define COCO-Style Dataset Loader ---\n",
        "class CarDDDataset(Dataset):\n",
        "    def __init__(self, annotation_path, image_dir, transforms=None):\n",
        "        self.coco = COCO(annotation_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_ids = self.coco.getImgIds()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            if ann['iscrowd'] == 1 or ann['bbox'][2] <= 1 or ann['bbox'][3] <= 1:\n",
        "                continue  # skip crowd or near-zero boxes\n",
        "            if ann['category_id'] == 6:\n",
        "                continue  # skip rare or invalid class\n",
        "            bbox = ann['bbox']\n",
        "            boxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, boxes, labels, img_id  # include ID if needed for logging or eval\n"
      ],
      "metadata": {
        "id": "TdHQG8_bXjPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Define Positional Encoding ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model) to match batch_first=True\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n"
      ],
      "metadata": {
        "id": "vZxqBzKmiePZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 6: Define Model Skeleton with CBAM ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "from positional_encoding import PositionalEncoding\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_planes // reduction, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(channels, reduction)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x * self.channel_attention(x)\n",
        "        out = out * self.spatial_attention(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CoDETR(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim=256, num_queries=100):\n",
        "        super().__init__()\n",
        "        assert num_classes > 0, \"num_classes must be greater than zero\"\n",
        "        assert num_queries > 0, \"num_queries must be greater than zero\"\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.register_buffer(\"valid_class_range\", torch.arange(num_classes))\n",
        "\n",
        "        self.backbone = timm.create_model('swin_base_patch4_window12_384', pretrained=True, features_only=True)\n",
        "        self.cbam = CBAM(self.backbone.feature_info[-1]['num_chs'])  # Inject CBAM\n",
        "        self.input_proj = nn.Conv2d(self.backbone.feature_info[-1]['num_chs'], hidden_dim, kernel_size=1)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
        "\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        self.bbox_embed = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 4)\n",
        "        )\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)[-1]                      # [B, H, W, C]\n",
        "        features = features.permute(0, 3, 1, 2).contiguous() # [B, C, H, W]\n",
        "        features = self.cbam(features)                       # Apply CBAM here\n",
        "        x = self.input_proj(features)                        # [B, hidden_dim, H, W]\n",
        "\n",
        "        bs, c, h, w = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)                     # (B, HW, C)\n",
        "        x = self.positional_encoding(x)\n",
        "        memory = self.encoder(x)                             # (B, HW, C)\n",
        "\n",
        "        queries = self.query_embed.weight.unsqueeze(0).repeat(bs, 1, 1)  # (B, num_queries, C)\n",
        "\n",
        "        if not hasattr(self, \"_logged_once\"):\n",
        "            print(f\"[DEBUG] features: {features.shape}, x: {x.shape}, memory: {memory.shape}, queries: {queries.shape}\")\n",
        "            self._logged_once = True\n",
        "\n",
        "        hs = self.decoder(queries, memory)  # (B, num_queries, C)\n",
        "        outputs_class = self.class_embed(hs)                 # (B, num_queries, num_classes)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()        # normalized boxes (B, num_queries, 4)\n",
        "\n",
        "        return {'pred_logits': outputs_class[:, -1], 'pred_boxes': outputs_coord[:, -1]}\n",
        "\n",
        "\n",
        "        # Pseudocode: Replace GIoU with SCYLLA-IoU by implementing a custom loss function per the paper.\n",
        "        # Suggested usage: replace criterion_bbox with scylla_iou_loss\n",
        "        # Example:\n",
        "        # def scylla_iou_loss(pred_boxes, target_boxes):\n",
        "        #     # Implement SCYLLA-IoU formula using angle, distance, aspect ratio components\n",
        "        #     return loss_value\n",
        "        # criterion_bbox = scylla_iou_loss\n",
        "        # See: scylla_iou_loss skeleton code file"
      ],
      "metadata": {
        "id": "Z8JGbCvUXmmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7: Training Loop and Evaluation ---\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch.nn.functional import cross_entropy, one_hot\n",
        "from torchvision.ops import generalized_box_iou, box_convert\n",
        "from tqdm import tqdm\n",
        "\n",
        "def hungarian_match(pred_logits, pred_boxes, tgt_labels, tgt_boxes):\n",
        "    with torch.no_grad():\n",
        "        if tgt_labels.numel() == 0 or pred_logits.numel() == 0 or pred_boxes.ndim != 2 or tgt_boxes.ndim != 2:\n",
        "            return [], []\n",
        "\n",
        "        num_queries = pred_logits.shape[0]\n",
        "        num_classes = pred_logits.shape[-1]\n",
        "        num_targets = tgt_labels.shape[0]\n",
        "\n",
        "        out_prob = pred_logits.softmax(-1)\n",
        "        out_bbox = pred_boxes\n",
        "\n",
        "        tgt_labels_onehot = one_hot(tgt_labels, num_classes=num_classes).float()\n",
        "        if tgt_labels_onehot.ndim == 1:\n",
        "            tgt_labels_onehot = tgt_labels_onehot.unsqueeze(0)\n",
        "\n",
        "        class_cost = -torch.matmul(out_prob, tgt_labels_onehot.T)\n",
        "        bbox_cost = torch.cdist(out_bbox, tgt_boxes, p=1)\n",
        "\n",
        "        out_bbox_xyxy = box_convert(out_bbox, 'cxcywh', 'xyxy')\n",
        "        tgt_bbox_xyxy = box_convert(tgt_boxes, 'cxcywh', 'xyxy')\n",
        "        giou = generalized_box_iou(out_bbox_xyxy, tgt_bbox_xyxy)\n",
        "        giou_cost = 1 - giou\n",
        "\n",
        "        total_cost = 1.0 * class_cost + 5.0 * bbox_cost + 2.0 * giou_cost\n",
        "        cost_matrix = total_cost.cpu().detach().numpy()\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "        return row_ind, col_ind\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion_cls, criterion_bbox, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (images, boxes, labels, img_ids) in enumerate(tqdm(dataloader, desc=\"Training\", leave=True, ncols=100)):\n",
        "        print(f\"[BATCH {batch_idx}] loading...\")\n",
        "        print(f\"  └─ box counts: {[len(b) for b in boxes]}, label counts: {[len(l) for l in labels]}\")\n",
        "        print(f\"[DEBUG] Labels this batch: {[l.tolist() for l in labels]}\")\n",
        "\n",
        "        if all(len(b) == 0 for b in boxes) or all(len(l) == 0 for l in labels):\n",
        "            print(f\"[SKIP] Batch {batch_idx} is empty — skipping.\")\n",
        "            continue\n",
        "\n",
        "        images = torch.stack(images).to(device)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        def compute_losses(logits, coords, boxes, labels):\n",
        "            all_cls_losses, all_bbox_losses = [], []\n",
        "            for i in range(len(images)):\n",
        "                if len(labels[i]) == 0 or coords[i].numel() == 0 or logits[i].numel() == 0:\n",
        "                    continue\n",
        "\n",
        "                pred_logits = logits[i]\n",
        "                pred_boxes = coords[i]\n",
        "                tgt_labels = labels[i]\n",
        "                tgt_boxes = box_convert(boxes[i], 'xyxy', 'cxcywh')\n",
        "\n",
        "                row_ind, col_ind = hungarian_match(pred_logits, pred_boxes, tgt_labels, tgt_boxes)\n",
        "                if len(row_ind) == 0 or len(col_ind) == 0:\n",
        "                    continue\n",
        "\n",
        "                matched_preds_logits = pred_logits[row_ind]\n",
        "                matched_preds_boxes = pred_boxes[row_ind]\n",
        "                matched_tgt_labels = tgt_labels[col_ind]\n",
        "                matched_tgt_boxes = tgt_boxes[col_ind]\n",
        "\n",
        "                cls_loss = criterion_cls(matched_preds_logits, matched_tgt_labels)\n",
        "                bbox_loss = criterion_bbox(matched_preds_boxes, matched_tgt_boxes)\n",
        "\n",
        "                all_cls_losses.append(cls_loss)\n",
        "                all_bbox_losses.append(bbox_loss)\n",
        "\n",
        "            return all_cls_losses, all_bbox_losses\n",
        "\n",
        "        main_cls_losses, main_bbox_losses = compute_losses(\n",
        "            outputs['pred_logits'], outputs['pred_boxes'], boxes, labels\n",
        "        )\n",
        "\n",
        "        aux_cls_losses, aux_bbox_losses = [], []\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for aux in outputs['aux_outputs']:\n",
        "                cls, bbox = compute_losses(aux['pred_logits'], aux['pred_boxes'], boxes, labels)\n",
        "                aux_cls_losses.extend(cls)\n",
        "                aux_bbox_losses.extend(bbox)\n",
        "\n",
        "        all_cls_losses = main_cls_losses + aux_cls_losses\n",
        "        all_bbox_losses = main_bbox_losses + aux_bbox_losses\n",
        "\n",
        "        if all_cls_losses:\n",
        "            loss_cls = torch.stack(all_cls_losses).mean()\n",
        "            loss_bbox = torch.stack(all_bbox_losses).mean()\n",
        "            loss = loss_cls + 5.0 * loss_bbox\n",
        "            print(f\"Backpropagating loss: {loss.item():.4f}\")\n",
        "            loss.backward()\n",
        "\n",
        "            # ✅ GRADIENT SANITY CHECK\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                    print(f\"[WARNING] NaN gradients in {name}\")\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, boxes, labels, img_ids in tqdm(dataloader, desc=\"Evaluating\", leave=True, ncols=100):\n",
        "            images = torch.stack(images).to(device)\n",
        "            labels = [l for l in labels if len(l) > 0]\n",
        "            if not labels:\n",
        "                continue\n",
        "            labels = torch.cat(labels).to(device)\n",
        "            outputs = model(images)\n",
        "            pred_classes = outputs['pred_logits'].argmax(-1).cpu()\n",
        "            all_preds.extend(pred_classes.view(-1).tolist())\n",
        "            all_targets.extend(labels.view(-1).tolist())\n",
        "\n",
        "    correct = sum([int(p == t) for p, t in zip(all_preds, all_targets)])\n",
        "    acc = correct / len(all_targets) if all_targets else 0\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "lmA4xcIhcL6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "gKU9RfRUXCKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 8: Main Training Script with Albumentations ---\n",
        "if __name__ == '__main__':\n",
        "    import torchvision.transforms as T\n",
        "    from sklearn.metrics import average_precision_score, roc_auc_score, precision_score, recall_score\n",
        "    from torch.nn.functional import one_hot\n",
        "    from collections import Counter\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import time\n",
        "    from tqdm import tqdm\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "    # Albumentations transform blocks\n",
        "    def get_train_transforms():\n",
        "        return A.Compose([\n",
        "            A.OneOf([\n",
        "                A.Mosaic(p=1.0),\n",
        "                A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=1.0),\n",
        "            ], p=0.7),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.HueSaturationValue(p=0.5),\n",
        "            A.RGBShift(p=0.3),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomResizedCrop(height=384, width=384, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=1.0),\n",
        "            A.Blur(blur_limit=3, p=0.3),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.3))\n",
        "\n",
        "    def get_val_transforms():\n",
        "        return A.Compose([\n",
        "            A.Resize(height=384, width=384),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "    # Albumentations-aware collate function\n",
        "    def albumentations_collate_fn(batch):\n",
        "        images, boxes, labels, img_ids = [], [], [], []\n",
        "        for img, box, label, img_id in batch:\n",
        "            transformed = get_train_transforms()(image=img, bboxes=box, labels=label)\n",
        "            images.append(transformed['image'])\n",
        "            boxes.append(torch.tensor(transformed['bboxes'], dtype=torch.float32))\n",
        "            labels.append(torch.tensor(transformed['labels'], dtype=torch.int64))\n",
        "            img_ids.append(img_id)\n",
        "        return images, boxes, labels, img_ids\n",
        "\n",
        "    # Hyperparameters\n",
        "    num_classes = 6\n",
        "    batch_size = 2\n",
        "    num_epochs = 1  # short for debugging\n",
        "    learning_rate = 1e-4\n",
        "    num_queries = 100\n",
        "    patience = 10\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else device)\n",
        "\n",
        "    # Dataset paths\n",
        "    train_json = os.path.join(extract_path, 'annotations/instances_train2017.json')\n",
        "    val_json = os.path.join(extract_path, 'annotations/instances_val2017.json')\n",
        "    train_img_dir = os.path.join(extract_path, 'train2017')\n",
        "    val_img_dir = os.path.join(extract_path, 'val2017')\n",
        "\n",
        "    # Datasets and loaders\n",
        "    train_dataset = CarDDDataset(train_json, train_img_dir)\n",
        "    val_dataset = CarDDDataset(val_json, val_img_dir)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=albumentations_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "    # ⚖️ Recalculate class frequencies after filtering\n",
        "    label_counter = Counter()\n",
        "    for _, _, labels, _ in DataLoader(train_dataset, batch_size=1, collate_fn=lambda x: tuple(zip(*x))):\n",
        "        for label_tensor in labels:\n",
        "            label_counter.update(label_tensor.tolist())\n",
        "    print(\"Actual class counts after filtering:\", label_counter)\n",
        "\n",
        "    class_counts = torch.tensor([label_counter.get(i, 0) for i in range(num_classes)], dtype=torch.float)\n",
        "    class_weights = 1.0 / (class_counts + 1e-6)\n",
        "    class_weights /= class_weights.sum()\n",
        "\n",
        "    # Model\n",
        "    model = CoDETR(num_classes=num_classes, num_queries=num_queries).to(device)\n",
        "\n",
        "    # Loss functions\n",
        "    class WeightedFocalLoss(nn.Module):\n",
        "        def __init__(self, alpha, gamma=2.0):\n",
        "            super().__init__()\n",
        "            self.alpha = alpha\n",
        "            self.gamma = gamma\n",
        "\n",
        "        def forward(self, inputs, targets):\n",
        "            ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha.to(inputs.device))\n",
        "            pt = torch.exp(-ce_loss)\n",
        "            focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "            if torch.rand(1).item() < 0.05:\n",
        "                print(f\"[FocalLoss] mean CE: {ce_loss.mean():.4f}, mean FL: {focal_loss.mean():.4f}\")\n",
        "            return focal_loss.mean()\n",
        "\n",
        "    criterion_cls = WeightedFocalLoss(alpha=class_weights, gamma=2.0)\n",
        "\n",
        "    from torchvision.ops import generalized_box_iou_loss, box_convert\n",
        "    criterion_bbox = lambda pred, target: generalized_box_iou_loss(\n",
        "        box_convert(pred, 'cxcywh', 'xyxy'),\n",
        "        box_convert(target, 'cxcywh', 'xyxy'),\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    best_map = 0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n"
      ],
      "metadata": {
        "id": "IAovXcgFdzlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (_, _, labels, _) in enumerate(train_loader):\n",
        "    for label_tensor in labels:\n",
        "        if label_tensor.max() >= num_classes:\n",
        "            print(f\"❌ Label index out of range in sample {i}: {label_tensor}\")\n"
      ],
      "metadata": {
        "id": "-2W5p536XMcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 9: Logging and Training Metrics Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_log = []\n",
        "acc_log = []\n",
        "\n",
        "def log_metrics(loss, acc):\n",
        "    loss_log.append(loss)\n",
        "    acc_log.append(acc)\n",
        "\n",
        "def plot_training_metrics(save_path=\"/content/drive/MyDrive/Colab Notebooks/cardd_output/training_metrics.png\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(loss_log, label=\"Loss\", color='red')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(acc_log, label=\"Accuracy\", color=\"green\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"📈 Training metrics plot saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "d4zGzYqeeQbH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}